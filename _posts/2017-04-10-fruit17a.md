---
title: Exploration-Exploitation in MDPs with Options
abstract: 'While a large body of empirical results show that temporally-extended actions
  and options may significantly affect the learning performance of an agent, the theoretical
  understanding of how and when options can be beneficial in online reinforcement
  learning is relatively limited. In this paper, we derive an upper and lower bound
  on the regret of a variant of UCRL using options. While we first analyze the algorithm
  in the general case of semi-Markov decision processes (SMDPs), we show how these
  results can be translated to the specific case of MDPs with options and we illustrate
  simple scenarios in which the regret of learning with options can be provably much
  smaller than the regret suffered when learning with primitive actions. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: fruit17a
month: 0
tex_title: "{Exploration-Exploitation in MDPs with Options}"
firstpage: 576
lastpage: 584
page: 576-584
order: 576
cycles: false
author:
- given: Ronan
  family: Fruit
- given: Alessandro
  family: Lazaric
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/fruit17a/fruit17a.pdf
extras:
- label: Supplementary pdf
  link: http://proceedings.mlr.press/v54/fruit17a/fruit17a/fruit17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
