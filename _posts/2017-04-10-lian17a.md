---
title: Finite-sum Composition Optimization via Variance Reduced Gradient Descent
abstract: 'The stochastic composition optimization proposed recently by Wang et al.
  [2014] minimizes the objective with the composite expectation form: $\min_x (\mathbbE_iF_i
  ∘\mathbbE_j G_j)(x).$ It summarizes many important applications in machine learning,
  statistics, and finance. In this paper, we consider the finite-sum scenario for
  composition optimization: $\min_x f (x) := \frac1n \sum_i = 1^n F_i \left(   \frac1m
  \sum_j = 1^m G_j (x) \right)$. In this paper, two algorithms are proposed to solve
  this problem by combining the stochastic compositional gradient descent (SCGD) and
  the stochastic variance reduced gradient (SVRG) technique. A constant linear convergence
  rate is proved for strongly convex optimization, which substantially improves the
  sublinear rate $O(K^-0.8)$ of the best known algorithm. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lian17a
month: 0
tex_title: "{Finite-sum Composition Optimization via Variance Reduced Gradient Descent}"
firstpage: 1159
lastpage: 1167
page: 1159-1167
order: 1159
cycles: false
author:
- given: Xiangru
  family: Lian
- given: Mengdi
  family: Wang
- given: Ji
  family: Liu
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/lian17a/lian17a.pdf
extras:
- label: Supplementary pdf
  link: http://proceedings.mlr.press/v54/lian17a/lian17a/lian17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
