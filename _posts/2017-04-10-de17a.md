---
title: Automated Inference with Adaptive Batches
abstract: Classical stochastic gradient methods for optimization rely on noisy gradient
  approximations that become progressively less accurate as iterates approach a solution.
  The large noise and small signal in the resulting gradients makes it difficult to
  use them for adaptive stepsize selection and automatic stopping. We propose alternative
  “big batch” SGD schemes that adaptively grow the batch size over time to maintain
  a nearly constant signal-to-noise ratio in the gradient approximation. The resulting
  methods have similar convergence rates to classical SGD, and do not require convexity
  of the objective. The high fidelity gradients enable automated learning rate selection
  and do not require stepsize decay. Big batch methods are thus easily automated and
  can run with little or no oversight.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: de17a
month: 0
tex_title: "{Automated Inference with Adaptive Batches}"
firstpage: 1504
lastpage: 1513
page: 1504-1513
order: 1504
cycles: false
author:
- given: Soham
  family: De
- given: Abhay
  family: Yadav
- given: David
  family: Jacobs
- given: Tom
  family: Goldstein
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/de17a/de17a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v54/de17a/de17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
