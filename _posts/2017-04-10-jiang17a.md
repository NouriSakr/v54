---
title: Combinatorial Topic Models using Small-Variance Asymptotics
abstract: Modern topic models typically have a probabilistic formulation, and derive
  their inference algorithms based on Latent Dirichlet Allocation (LDA) and its variants.
  In contrast, we approach topic modeling via combinatorial optimization, and take
  a small-variance limit of LDA to derive a new objective function. We minimize this
  objective by using ideas from combinatorial optimization, obtaining a new, fast,
  and high-quality topic modeling algorithm.  In particular, we show that our results
  are not only significantly better than traditional SVA algorithms, but also truly
  competitive with popular LDA-based approaches; we also discuss the (dis)similarities
  between our approach and its probabilistic counterparts.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jiang17a
month: 0
tex_title: "{Combinatorial Topic Models using Small-Variance Asymptotics}"
firstpage: 421
lastpage: 429
page: 421-429
order: 421
cycles: false
author:
- given: Ke
  family: Jiang
- given: Suvrit
  family: Sra
- given: Brian
  family: Kulis
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/jiang17a/jiang17a.pdf
extras:
- label: Supplementary pdf
  link: http://proceedings.mlr.press/v54/jiang17a/jiang17a/jiang17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
