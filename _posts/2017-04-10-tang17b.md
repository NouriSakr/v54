---
title: Convergence rate of stochastic k-means
abstract: We analyze online (Bottou & Bengio, 1994) and mini-batch (Sculley, 2010)
  k-means variants. Both scale up the widely used Lloyd’s algorithm via stochastic
  approximation, and have become popular for large-scale clustering and unsupervised
  feature learning. We show, for the first time, that they have global convergence
  towards “local optima” at rate O(1/t) under general conditions. In addition, we
  show that if the dataset is clusterable, stochastic k-means with suitable initialization
  converges to an optimal k-means solution at rate O(1/t) with high probability. The
  k-means objective is non-convex and non-differentiable; we exploit ideas from non-convex
  gradient-based optimization by providing a novel characterization of the trajectory
  of the k-means algorithm on its solution space, and circumvent its non-differentiability
  via geometric insights about the k-means update.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: tang17b
month: 0
tex_title: "{Convergence rate of stochastic k-means}"
firstpage: 1495
lastpage: 1503
page: 1495-1503
order: 1495
cycles: false
author:
- given: Cheng
  family: Tang
- given: Claire
  family: Monteleoni
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/tang17b/tang17b.pdf
extras:
- label: Supplementary pdf
  link: http://proceedings.mlr.press/v54/tang17b/tang17b/tang17b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
