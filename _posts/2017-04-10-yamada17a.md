---
title: Localized Lasso for High-Dimensional Regression
abstract: We introduce the localized Lasso, which learns models that both are interpretable
  and have a high predictive power in problems with high dimensionality d and small
  sample size n.  More specifically, we consider a function defined by local sparse
  models, one at each data point. We introduce sample-wise network regularization
  to borrow strength across the models, and sample-wise exclusive group sparsity (a.k.a.,
  l12 norm) to introduce diversity into the choice of feature sets in the local models.
  The local models are interpretable in terms of similarity of their sparsity patterns.
  The cost function is convex, and thus has a globally optimal solution. Moreover,
  we propose a simple yet efficient iterative least-squares based optimization procedure
  for the localized Lasso, which does not need a tuning parameter, and is guaranteed
  to converge to a globally optimal solution. The solution is empirically shown to
  outperform alternatives for both simulated and genomic personalized/precision medicine
  data.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yamada17a
month: 0
tex_title: "{Localized Lasso for High-Dimensional Regression}"
firstpage: 325
lastpage: 333
page: 325-333
order: 325
cycles: false
author:
- given: Makoto
  family: Yamada
- given: Takeuchi
  family: Koh
- given: Tomoharu
  family: Iwata
- given: John
  family: Shawe-Taylor
- given: Samuel
  family: Kaski
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/yamada17a/yamada17a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v54/yamada17a/yamada17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
