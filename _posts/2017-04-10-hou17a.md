---
title: ConvNets with Smooth Adaptive Activation Functions for Regression
abstract: Within Neural Networks (NN), the parameters of Adaptive Activation Functions
  (AAF) control the shapes of activation functions. These parameters are trained along
  with other parameters in the NN. AAFs have improved performance of Convolutional
  Neural Networks (CNN) in multiple classification tasks. In this paper, we propose
  and apply AAFs on CNNs for regression tasks. We argue that applying AAFs in the
  regression (second-to-last) layer of a NN can significantly decrease the bias of
  the regression NN. However, using existing AAFs may lead to overfitting. To address
  this problem, we propose a Smooth Adaptive Activation Function (SAAF) with a piecewise
  polynomial form which can approximate any continuous function to arbitrary degree
  of error, while having a bounded Lipschitz constant for given bounded model parameters.
  As a result, NNs with SAAF can avoid overfitting by simply regularizing model parameters.
  We empirically evaluated CNNs with SAAFs and achieved state-of-the-art results on
  age and pose estimation datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hou17a
month: 0
tex_title: "{ConvNets with Smooth Adaptive Activation Functions for Regression}"
firstpage: 430
lastpage: 439
page: 430-439
order: 430
cycles: false
author:
- given: Le
  family: Hou
- given: Dimitris
  family: Samaras
- given: Tahsin
  family: Kurc
- given: Yi
  family: Gao
- given: Joel
  family: Saltz
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/hou17a/hou17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
