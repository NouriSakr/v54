---
title: Online Optimization of Smoothed Piecewise Constant Functions
abstract: We study online optimization of smoothed piecewise constant functions over
  the domain [0, 1). This is motivated by the problem of adaptively picking parameters
  of learning algorithms as in the recently introduced framework by Gupta and Roughgarden
  (2016). Majority of the machine learning literature has focused on Lipschitz-continuous
  functions or functions with bounded gradients. This is with good reason-any learning
  algorithm suffers linear regret even against piecewise constant functions that are
  chosen adversarially, arguably the simplest of non-Lipschitz continuous functions.
  The smoothed setting we consider is inspired by the seminal work of Spielman and
  Teng (2004) and the recent work of Gupta and Roughgarden (2016)-in this setting,
  the sequence of functions may be chosen by an adversary, however, with some uncertainty
  in the location of discontinuities. We give algorithms that achieve sublinear regret
  in the full information and bandit settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cohen-addad17a
month: 0
tex_title: "{Online Optimization of Smoothed Piecewise Constant Functions}"
firstpage: 412
lastpage: 420
page: 412-420
order: 412
cycles: false
author:
- given: Vincent
  family: Cohen-Addad
- given: Varun
  family: Kanade
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/cohen-addad17a/cohen-addad17a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
