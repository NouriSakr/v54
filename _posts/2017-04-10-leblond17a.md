---
title: 'ASAGA: Asynchronous Parallel SAGA'
abstract: We describe ASAGA, an asynchronous parallel version of the incremental gradient
  algorithm SAGA that enjoys fast linear convergence rates. Through a novel perspective,
  we revisit and clarify a subtle but important technical issue present in a large
  fraction of the recent convergence rate proofs for asynchronous parallel optimization
  algorithms, and propose a simplification of the recently introduced “perturbed iterate”
  framework that resolves it. We thereby prove that ASAGA can obtain a theoretical
  linear speedup on multi-core systems even without sparsity assumptions. We present
  results of an implementation on a 40-core architecture illustrating the practical
  speedup as well as the hardware overhead.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: leblond17a
month: 0
tex_title: "{ASAGA: Asynchronous Parallel SAGA}"
firstpage: 46
lastpage: 54
page: 46-54
order: 46
cycles: false
author:
- given: Rémi
  family: Leblond
- given: Fabian
  family: Pedregosa
- given: Simon
  family: Lacoste-Julien
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/leblond17a/leblond17a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v54/leblond17a/leblond17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
