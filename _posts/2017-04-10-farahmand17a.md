---
title: Value-Aware Loss Function for Model-based Reinforcement Learning
abstract: We consider the problem of estimating the transition probability kernel
  to be used by a model-based reinforcement learning (RL) algorithm. We argue that
  estimating a generative model that minimizes a probabilistic loss, such as the log-loss,
  is an overkill because it does not take into account the underlying structure of
  decision problem and the RL algorithm that intends to solve it. We introduce a loss
  function that takes the structure of the value function into account. We provide
  a finite-sample upper bound for the loss function showing the dependence of the
  error on model approximation error, number of samples, and the complexity of the
  model space. We also empirically compare the method with the maximum likelihood
  estimator on a simple problem.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: farahmand17a
month: 0
tex_title: "{Value-Aware Loss Function for Model-based Reinforcement Learning}"
firstpage: 1486
lastpage: 1494
page: 1486-1494
order: 1486
cycles: false
author:
- given: Amir-Massoud
  family: Farahmand
- given: Andre
  family: Barreto
- given: Daniel
  family: Nikovski
date: 2017-04-10
address: 
publisher: PMLR
container-title: Proceedings of the 20th International Conference on Artificial Intelligence
  and Statistics
volume: '54'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 4
  - 10
pdf: http://proceedings.mlr.press/v54/farahmand17a/farahmand17a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v54/farahmand17a/farahmand17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
